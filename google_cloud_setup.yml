version: '3.1'
networks:
  proxy: {external: true}
services:
  backend:
    command: gunicorn -b 0.0.0.0:8000 "manage:uggipuggi_app.app"
    depends_on: [logstash, statsd, redis, mongo, celery, kafka, zookeeper, kafka_consumer, kafka_subscriber_activity]
    deploy:
      labels: [com.df.notify=true, com.df.distribute=true, com.df.servicePath=/,
        com.df.port=8000]
    image: kr:latest
    networks: [proxy]
    ports: ['8000:8000']
    volumes: ['/home/uggi_puggi_krishna/images:/images']
  celery:
    command: celery worker -l debug -A uggipuggi.celery.celery -n worker.high -Q high
    depends_on: [redis, mongo]
    environment: {CELERY_BROKER_URL: 'redis://redis:6379/0', CELERY_RESULT_BACKEND: 'redis://redis:6379/0',
      KAFKA_BOOTSTRAP_SERVERS: 'kafka:9092'}
    image: kr:latest
    networks: [proxy]
    ports: ['8002:8000']
    volumes: ['/home/uggi_puggi_krishna/images:/images']
  kafka:
    depends_on: [zookeeper]
    environment: {KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:9092', KAFKA_BROKER_ID: 1,
      KAFKA_LOG_DIRS: /var/logs/kafka, KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1,
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'}
    image: confluentinc/cp-kafka:latest
    networks: [proxy]
    ports: ['9092:9092']
  kafka_consumer:
    command: python3 uggipuggi/messaging/subscriber/recipe_kafka_collection_post_subscriber.py
    depends_on: [kafka, redis, zookeeper]
    environment: {KAFKA_BOOTSTRAP_SERVERS: 'kafka:9092'}
    image: kr:latest
    networks: [proxy]
    ports: ['8001:8000']
  kafka_subscriber_activity:
    command: python3 uggipuggi/messaging/subscriber/activity_kafka_collection_post_subscriber.py
    depends_on: [kafka, redis, zookeeper]
    environment: {KAFKA_BOOTSTRAP_SERVERS: 'kafka:9092'}
    image: kr:latest
    networks: [proxy]
    ports: ['8003:8000']
  mongo:
    environment: {MONGO_DATA_DIR: /data/db, MONGO_LOG_DIR: /dev/null}
    image: mongo:latest
    networks: [proxy]
    ports: ['27017:27017']
  proxy:
    deploy:
      mode: global
      placement:
        constraints: [node.role == manager]
    environment: [LISTENER_ADDRESS=swarm-listener, MODE=swarm, 'CONNECTION_MODE=${CONNECTION_MODE:-http-keep-alive}']
    image: vfarcic/docker-flow-proxy:${TAG:-latest}
    networks: [proxy]
    ports: ['80:80']
  redis:
    image: redis:latest
    networks: [proxy]
    ports: ['6379:6379']
  swarm-listener:
    deploy:
      mode: global
      placement:
        constraints: [node.role == manager]
    environment: ['DF_NOTIFY_CREATE_SERVICE_URL=http://proxy:8080/v1/docker-flow-proxy/reconfigure',
      'DF_NOTIFY_REMOVE_SERVICE_URL=http://proxy:8080/v1/docker-flow-proxy/remove']
    image: vfarcic/docker-flow-swarm-listener
    networks: [proxy]
    volumes: ['/var/run/docker.sock:/var/run/docker.sock']
  visualizer:
    deploy:
      labels: [com.df.notify=true, com.df.distribute=true, com.df.servicePath=/visualizer,
        com.df.port=8080]
      placement:
        constraints: [node.role == manager]
      replicas: 1
    environment: [CTX_ROOT=/visualizer]
    image: dockersamples/visualizer:stable
    networks: [proxy]
    stop_grace_period: 1m30s
    volumes: ['/var/run/docker.sock:/var/run/docker.sock']
  zookeeper:
    environment: {ZOOKEEPER_CLIENT_PORT: 2181, ZOOKEEPER_SERVER_ID: 1, ZOOKEEPER_TICK_TIME: 2000}
    image: confluentinc/cp-zookeeper:latest
    networks: [proxy]
    ports: ['2181:2181']

  rabbitmq:
    image: deadtrickster/rabbitmq_prometheus
    depends_on: [prometheus]
    ports:
      - "5673:5672"
      - "15673:15672"
    networks: [proxy]

  statsd-exporter:
    image: prom/statsd-exporter:latest
    command: "--statsd.mapping-config=/tmp/statsd_mapping.yml"
    depends_on: [prometheus]
    ports:
      - "9102:9102"
      - "9125:9125/udp"
    networks: [proxy]
    volumes:
      - "./metrics/statsd_mapping.yml:/tmp/statsd_mapping.yml"

  prometheus:
    image: prom/prometheus
    command: "--config.file=/tmp/prometheus.yml --web.listen-address '0.0.0.0:9090'"
    ports:
      - "9090:9090"
    networks: [proxy]
    volumes:
      - "./metrics/prometheus.yml:/tmp/prometheus.yml"

  grafana:
    image: grafana/grafana
    depends_on: [prometheus]
    volumes:
      - "./grafana/grafana.ini:/etc/grafana/grafana.ini"
    ports:
      - "3900:3000"
    networks: [proxy]

  kibana:
    image: up_kibana:latest
    volumes:
      - ./kibana/config/:/usr/share/kibana/config:ro
    ports:
      - "5601:5601"
    networks: [proxy]
    depends_on:
      - elasticsearch

  # localhost:9200/_cat/indices?v
  elasticsearch:
    image: up_es:latest
    volumes:
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    ports:
      - "9200:9200"
      - "9300:9300"
    networks: [proxy]
    ulimits:
      memlock:
        soft: -1
        hard: -1
    environment:
      - 'discovery.type=single-node'
      - 'bootstrap.memory_lock=true'
      - 'ES_JAVA_OPTS=-Xms512m -Xmx512m'

  logstash:
    image: up_ls:latest
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
    ports:
      - "5000:5000"
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks: [proxy]
    depends_on:
      - elasticsearch

  statsd:
    image: dockerana/statsd
    depends_on: [statsd-exporter]
    ports:
      - "8125:8125/udp"
      - "8126:8126"
    networks: [proxy]
    volumes:
      - "./metrics/statsd_config.js:/src/statsd/config.js"

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "9411:9411"
    networks: [proxy]
    environment:
      COLLECTOR_ZIPKIN_HTTP_PORT: "9411"